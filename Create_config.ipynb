{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from logging import getLogger\n",
    "from recbole.config import Config\n",
    "from recbole.utils import init_seed, init_logger\n",
    "from recbole.data import create_dataset, data_preparation\n",
    "from recbole.utils import get_model, get_trainer\n",
    "from recbole.trainer import HyperTuning\n",
    "from recbole.quick_start import objective_function\n",
    "\n",
    "SEED = 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_dict = {\n",
    "        \n",
    "    # environment\n",
    "    'seed': SEED,\n",
    "    'reproducibility': True,\n",
    "    'data_path': 'dataset/collections/',\n",
    "    'checkpoint_dir': 'saved/',\n",
    "    'show_progress': True,\n",
    "    'save_dataset': False,\n",
    "    'log_wandb': True,\n",
    "    'save_dataloaders': True,\n",
    "    'dataloaders_save_path': 'dataloader/',\n",
    "    \n",
    "    # data\n",
    "    'field_separator': '\\t',\n",
    "    'seq_separator': ' ',\n",
    "    'USER_ID_FIELD': 'user_id',\n",
    "    'ITEM_ID_FIELD': 'item_id',\n",
    "    'RATING_FIELD': 'rating',\n",
    "    'item_inter_num_interval': '[0,inf)', \n",
    "    \n",
    "    # training\n",
    "    'epochs': 50,\n",
    "    'train_batch_size': 2048, # 2048\n",
    "    'learner': 'adam',\n",
    "    'learning_rate': 0.001, # 0.001\n",
    "    'train_neg_sample_args': {'distribution': 'popularity',\n",
    "                              'sample_num': 5,\n",
    "                              'dynamic': False,\n",
    "                              'candidate_num': 0},\n",
    "    'eval_step': 1,\n",
    "    'stopping_step': 3000, # 15\n",
    "    'loss_decimal_place': 4,\n",
    "    \n",
    "    # evaluation\n",
    "    'eval_args': {'group_by': 'user',\n",
    "                  'order': 'RO',\n",
    "                  'split': {'RS':[8,1,1]},\n",
    "                  'mode': 'pop100'},\n",
    "    'metrics': ['Recall', 'MRR', 'NDCG', 'Hit', 'MAP', 'Precision', 'GAUC'],\n",
    "    'topk': [1, 2, 5, 10, 20], \n",
    "    'valid_metric': 'NDCG@20', # for early stopping\n",
    "    'eval_batch_size': 4096, # 4096\n",
    "    'metric_decimal_place': 4\n",
    "    \n",
    "}\n",
    "\n",
    "# convert parameter_dict to yaml file\n",
    "with open(r'config/fixed_config_baseline.yaml', 'w') as file:\n",
    "    documents = yaml.dump(parameter_dict, file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Run models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_names = ['BPR'] #['BPR', 'DMF', 'NeuMF', 'NGCF', 'LightGCN'] \n",
    "DATASET_names = ['bayc']\n",
    "ITEM_CUT_list = [3]\n",
    "\n",
    "result_path = './result/'\n",
    "if not os.path.exists(result_path):\n",
    "    os.makedirs(result_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]20 Feb 19:28    WARNING  In inter_feat, line [7600], user_id do not exist, so they will be removed.\n",
      "20 Feb 19:28    INFO  bayc\n",
      "The number of users: 529\n",
      "Average actions of users: 5.087121212121212\n",
      "The number of items: 1958\n",
      "Average actions of items: 1.372508942258559\n",
      "The number of inters: 2686\n",
      "The sparsity of the dataset: 99.74067902319213%\n",
      "Remain Fields: ['user_id', 'item_id']\n",
      "20 Feb 19:28    INFO  Saving split dataloaders into: [saved/bayc-for-BPR-dataloader.pth]\n",
      "20 Feb 19:28    INFO  [Training]: train_batch_size = [2048] train_neg_sample_args: [{'candidate_num': 0, 'distribution': 'popularity', 'dynamic': False, 'sample_num': 5, 'alpha': 1.0}]\n",
      "20 Feb 19:28    INFO  [Evaluation]: eval_batch_size = [4096] eval_args: [{'group_by': 'user', 'mode': 'pop100', 'order': 'RO', 'split': {'RS': [8, 1, 1]}}]\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.45it/s]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    for MODEL in tqdm(MODEL_names):\n",
    "        test_result_list = []\n",
    "        for DATASET in DATASET_names:\n",
    "            for ITEM_CUT in ITEM_CUT_list:\n",
    "            \n",
    "                config = Config(model=MODEL, dataset=DATASET, config_file_list=['config/fixed_config_baseline.yaml'])\n",
    "                config['user_inter_num_interval'] = f'[{ITEM_CUT},inf)'\n",
    "                \n",
    "                # init random seed\n",
    "                init_seed(config['seed'], config['reproducibility'])\n",
    "\n",
    "                # logger initialization\n",
    "                init_logger(config)\n",
    "                logger = getLogger()\n",
    "\n",
    "                # write config info into log\n",
    "                # logger.info(config) # print config info\n",
    "\n",
    "                # dataset creating and filtering # convert atomic files -> Dataset\n",
    "                dataset = create_dataset(config)\n",
    "                logger.info(dataset) # print dataset info\n",
    "\n",
    "                # dataset splitting # convert Dataset -> Dataloader\n",
    "                train_data, valid_data, test_data = data_preparation(config, dataset)\n",
    "\n",
    "        #         # model loading and initialization\n",
    "        #         model = get_model(config['model'])(config, train_data.dataset).to(config['device'])\n",
    "        #         logger.info(model)\n",
    "\n",
    "        #         # trainer loading and initialization\n",
    "        #         trainer = get_trainer(config['MODEL_TYPE'], config['model'])(config, model)\n",
    "                \n",
    "\n",
    "        #         \"\"\" (1) training \"\"\"\n",
    "\n",
    "        #         # # resume from break point\n",
    "        #         # checkpoint_file = 'checkpoint.pth'\n",
    "        #         # trainer.resume_checkpoint(checkpoint_file)\n",
    "                \n",
    "        #         # model training\n",
    "        #         best_valid_score, best_valid_result = trainer.fit(train_data, valid_data)\n",
    "\n",
    "\n",
    "        #         \"\"\" (2) testing \"\"\"\n",
    "\n",
    "        #         # When calculate ItemCoverage metrics, we need to run this code for set item_nums in eval_collector.\n",
    "        #         trainer.eval_collector.data_collect(train_data)\n",
    "\n",
    "        #         # model evaluation\n",
    "        #         checkpoint_file = get_last_file('./saved/')\n",
    "        #         print(checkpoint_file)\n",
    "        #         test_result = trainer.evaluate(test_data, model_file=checkpoint_file)\n",
    "        #         print('FINAL TEST RESULT')\n",
    "        #         print(test_result)\n",
    "        #         test_result_list.append(pd.DataFrame.from_dict(test_result, orient='index', columns=[DATASET+'_'+str(ITEM_CUT)]))\n",
    "                \n",
    "        # pd.concat(test_result_list, axis=1).to_csv(result_path + f'{MODEL}.csv', index=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: HPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function(config_dict=None, config_file_list=None):\n",
    "    \n",
    "    config = Config(model=MODEL, dataset=DATASET, config_dict=config_dict, config_file_list=config_file_list)\n",
    "    init_seed(config['seed'], config['reproducibility'])\n",
    "    dataset = create_dataset(config)\n",
    "    train_data, valid_data, test_data = data_preparation(config, dataset)\n",
    "    model_name = config['model']\n",
    "    model = get_model(model_name)(config, train_data.dataset).to(config['device'])\n",
    "    trainer = get_trainer(config['MODEL_TYPE'], config['model'])(config, model)\n",
    "    \"\"\" (1) training \"\"\"\n",
    "    best_valid_score, best_valid_result = trainer.fit(train_data, valid_data, verbose=False)\n",
    "    \"\"\" (2) testing \"\"\"\n",
    "    test_result = trainer.evaluate(test_data)\n",
    "\n",
    "    return {\n",
    "        'model': model_name,\n",
    "        'best_valid_score': best_valid_score,\n",
    "        'valid_score_bigger': config['valid_metric_bigger'],\n",
    "        'best_valid_result': best_valid_result,\n",
    "        'test_result': test_result\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_names = ['NGCF']\n",
    "DATASET_names = ['meebits']\n",
    "ITEM_CUT_list = [3]\n",
    "\n",
    "result_path = './result/'\n",
    "# create folder result_path\n",
    "if not os.path.exists(result_path):\n",
    "    os.makedirs(result_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MODEL_names' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18296\\795344120.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mMODEL\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mMODEL_names\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mDATASET\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDATASET_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mHPO_test_result_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mITEM_CUT\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mITEM_CUT_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'MODEL_names' is not defined"
     ]
    }
   ],
   "source": [
    "for MODEL in MODEL_names:\n",
    "    for DATASET in tqdm(DATASET_names):\n",
    "        HPO_test_result_list = []\n",
    "        for ITEM_CUT in ITEM_CUT_list:\n",
    "            \n",
    "            hp = HyperTuning(objective_function=objective_function, algo='exhaustive', \n",
    "                                max_evals=50, params_file=f'hyper/{MODEL}.hyper', fixed_config_file_list=['config/fixed_config_baseline.yaml'])\n",
    "\n",
    "            # run\n",
    "            hp.run()\n",
    "            # export result to the file\n",
    "            hp.export_result(output_file=f'hyper/{MODEL}_{DATASET}_{ITEM_CUT}.result')\n",
    "            # print best parameters\n",
    "            print('best params: ', hp.best_params)\n",
    "            # save best parameters\n",
    "            with open(f'hyper/{MODEL}_{DATASET}_{ITEM_CUT}.best_params', 'w') as file:\n",
    "                documents = yaml.dump(hp.best_params, file)\n",
    "            # print best result\n",
    "            best_result = hp.params2result[hp.params2str(hp.best_params)]\n",
    "            print('best result: ')\n",
    "            print(best_result)\n",
    "            \n",
    "            HPO_test_result_list.append(pd.DataFrame.from_dict(best_result['test_result'], orient='index', columns=[f'{DATASET}_{ITEM_CUT}'])) \n",
    "        \n",
    "        pd.concat(HPO_test_result_list, axis=1).to_csv(result_path + f'{MODEL}_{DATASET}_{ITEM_CUT}.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RecBole_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "991fe3f9de00c9a422a5f66b8cc7243158afe66a42c9654a2fcf9d740859f175"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
